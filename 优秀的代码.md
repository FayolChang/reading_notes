# 优秀的代码

```python
# https://www.kaggle.com/fxarte/sentiment-model-with-sklearn

def plot_significant_features(pipeline=None, n=20):
    feature_names = pipeline.get_params()['vect'].get_feature_names()
    coefs=[]
    try:
        coefs = pipeline.get_params()['clf'].coef_
    except:
        coefs.append(pipeline.get_params()['clf'].feature_importances_)
    
    print("Total features: {}".format(len(coefs[0])))
    coefs_with_fns = sorted(zip(coefs[0], feature_names))
    top = coefs_with_fns[:-(n + 1):-1]
    
    y,X = zip(*top)

    plt.figure()
    plt.title("Top 20 most important features")
    plt.gcf().subplots_adjust(bottom=0.25)
    ax = plt.subplot(111)
    
    ax.bar(range(len(X)), y, color="r", align="center")
    ax.set_xticks(range(len(X)))
    ax.set_xlim(-1, len(X))
    ax.set_xticklabels(X,rotation='vertical')
    plt.savefig('sentiment_feature_importance.png')
    plt.close()
```

下面的代码是自定义一个Transformer, 可以用来生成一些特征：

```python
# http://zacstewart.com/2014/08/05/pipelines-of-featureunions-of-pipelines.html
class ModelTransformer(TransformerMixin):

    def __init__(self, model):
        self.model = model

    def fit(self, *args, **kwargs):
        self.model.fit(*args, **kwargs)
        return self

    def transform(self, X, **transform_params):
        return DataFrame(self.model.predict(X))
    
class HourOfDayTransformer(TransformerMixin):

    def transform(self, X, **transform_params):
        hours = DataFrame(X['datetime'].apply(lambda x: x.hour))
        return hours

    def fit(self, X, y=None, **fit_params):
        return self
```

原文如此，但是scikit-learn的标准做法是继承自BaseEstimator和TransformerMixin.

然后就可以把Pipeline写的很复杂：

```python
pipeline = Pipeline([
    ('features', FeatureUnion([
        ('continuous', Pipeline([
            ('extract', ColumnExtractor(CONTINUOUS_FIELDS)),
            ('scale', Normalizer())
        ])),
        ('factors', Pipeline([
            ('extract', ColumnExtractor(FACTOR_FIELDS)),
            ('one_hot', OneHotEncoder(n_values=5)),
            ('to_dense', DenseTransformer())
        ])),
        ('weekday', Pipeline([
            ('extract', DayOfWeekTransformer()),
            ('one_hot', OneHotEncoder()),
            ('to_dense', DenseTransformer())
        ])),
        ('hour_of_day', HourOfDayTransformer()),
        ('month', Pipeline([
            ('extract', ColumnExtractor(['datetime'])),
            ('to_month', DateTransformer()),
            ('one_hot', OneHotEncoder()),
            ('to_dense', DenseTransformer())
        ])),
        ('growth', Pipeline([
            ('datetime', ColumnExtractor(['datetime'])),
            ('to_numeric', MatrixConversion(int)),
            ('regression', ModelTransformer(LinearRegression()))
        ]))
    ])),
    ('estimators', FeatureUnion([
        ('knn', ModelTransformer(KNeighborsRegressor(n_neighbors=5))),
        ('gbr', ModelTransformer(GradientBoostingRegressor())),
        ('dtr', ModelTransformer(DecisionTreeRegressor())),
        ('etr', ModelTransformer(ExtraTreesRegressor())),
        ('rfr', ModelTransformer(RandomForestRegressor())),
        ('par', ModelTransformer(PassiveAggressiveRegressor())),
        ('en', ModelTransformer(ElasticNet())),
        ('cluster', ModelTransformer(KMeans(n_clusters=2)))
    ])),
    ('estimator', KNeighborsRegressor())
])
```

jr preprocessing 也有很好的自定义Transformer

```python
# https://raw.githubusercontent.com/kingjr/jr-tools/master/jr/gat/preprocessing.py
import numpy as np
from sklearn.base import BaseEstimator, TransformerMixin


class Averager(BaseEstimator, TransformerMixin):
    """Average data set into n samples"""
    def __init__(self, n, mean=None):
        if mean is None:
            mean = np.mean
        self.n = int(n)
        self.mean = mean

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        if len(X) <= self.n:
            return X
        Xt = np.zeros((self.n, X.shape[1]))
        y = np.round(np.arange(len(X)) / float(len(X)) * self.n)
        self.y_ = y
        for ii in range(self.n):
            sel = np.where(y == ii)[0]
            Xt[ii, ...] = self.mean(X[sel, ...], axis=0)
        return Xt


class MeanFeatures(BaseEstimator, TransformerMixin):
    """Reshape and mean data along given axis."""
    def __init__(self, shape, axis):
        self.axis = axis
        self.shape = shape

    def fit(self, X, y=None):
        pass

    def fit_transform(self, X, y=None):
        return self.transform(X, y)

    def transform(self, X, y=None):
        X = np.reshape(X, np.hstack((X.shape[0], self.shape)))
        return np.mean(X, axis=self.axis)


class DigitizedTransformer(BaseEstimator, TransformerMixin):
    def __init__(self, bins, estimator):
        self.bins = bins
        self.estimator = estimator

    def fit(self, X, y):
        y_bin = np.digitize(y, self.bins)
        self.estimator.fit(X, y=y_bin)
        return self

    def transform(self, X):
        return self.estimator.transform(X)


class GenericTransformer(BaseEstimator, TransformerMixin):
    def __init__(self, function, **fit_params):
        self.function = function
        self.fit_params = fit_params

    def fit(self, X, y=None):
        pass

    def transform(self, X, y=None):
        return self.function(X, **self.fit_params)

    def fit_transform(self, X, y=None):
        return self.transform(X, y)
```

其中DigitizedTransformer和GenericTransformer应该会很常用。



Far0n 写了一个kaggletils(https://github.com/Far0n/kaggletils). 其中的encoders.py里面的Transformer都是比赛常用的。

```python
# -*- coding: utf-8 -*-
"""
@author: Mathias Müller | Faron - kaggle.com/mmueller
"""
# https://raw.githubusercontent.com/Far0n/kaggletils/master/kaggletils/preprocessing/encoders.py
from __future__ import division

from collections import Counter

import numpy as np
from scipy.stats import norm
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.preprocessing import LabelEncoder
from statsmodels.distributions import ECDF

from ..estimators import LikelihoodEstimator
from ..utils.data import is_numpy


class CountEncoder(BaseEstimator, TransformerMixin):
    def __init__(self, min_count=0, nan_value=-1, copy=True):
        self.min_count = min_count
        self.nan_value = nan_value
        self.copy = copy
        self.counts = {}

    def fit(self, x):
        self.counts = {}
        if len(x.shape) == 1:
            x = x.reshape(-1, 1)
        ncols = x.shape[1]
        is_np = is_numpy(x)

        for i in range(ncols):
            if is_np:
                cnt = dict(Counter(x[:, i]))
            else:
                cnt = x.iloc[:, i].value_counts().to_dict()
            if self.min_count > 0:
                cnt = dict((k, self.nan_value if v < self.min_count else v) for k, v in cnt.items())
            self.counts.update({i: cnt})
        return self

    def fit_transform(self, x):
        self.fit(x)
        return self.transform(x)

    def transform(self, x):
        if self.copy:
            x = x.copy()
        if len(x.shape) == 1:
            x = x.reshape(-1, 1)
        ncols = x.shape[1]
        is_np = is_numpy(x)

        for i in range(ncols):
            cnt = self.counts[i]
            if is_np:
                k, v = np.array(list(zip(*sorted(cnt.items()))))
                ix = np.digitize(x[:, i], k, right=True)
                x[:, i] = v[ix]
            else:
                x.iloc[:, i].replace(cnt, inplace=True)
        return x


class LikelihoodEncoder(BaseEstimator, TransformerMixin):
    def __init__(self, seed=0, alpha=0, leave_one_out=False, noise=0):
        self.alpha = alpha
        self.noise = noise
        self.seed = seed
        self.leave_one_out = leave_one_out
        self.nclass = None
        self.estimators = []

    def fit(self, x, y):
        if len(x.shape) == 1:
            x = x.reshape(-1, 1)
        ncols = x.shape[1]
        if not is_numpy(x):
            x = np.array(x)

        self.nclass = np.unique(y).shape[0]

        for i in range(ncols):
            self.estimators.append(LikelihoodEstimator(**self.get_params()).fit(x[:, i], y))
        return self

    @staticmethod
    def owen_zhang(x_train, y_train, x_test, seed=0, alpha=0, noise=0.01):
        """
        Owen Zhang's leave-one-out + noise likelihood encoding

        "Winning data science competitions"
        http://de.slideshare.net/ShangxuanZhang/winning-data-science-competitions-presented-by-owen-zhang
        """
        if len(x_train.shape) == 1:
            x_train = x_train.reshape(-1, 1)
            x_test = x_test.reshape(-1, 1)
        ncols = x_train.shape[1]
        nclass = np.unique(y_train).shape[0]
        if not is_numpy(x_train):
            x_train = np.array(x_train)
            x_test = np.array(x_test)

        xx_train = None
        xx_test = None

        for i in range(ncols):
            le_train = LikelihoodEstimator(noise=noise, alpha=alpha, leave_one_out=True, seed=seed). \
                fit(x_train[:, i], y_train)
            le_test = LikelihoodEstimator(noise=0, alpha=alpha, leave_one_out=False, seed=seed). \
                fit(x_train[:, i], y_train)
            lh_train = le_train.x_likelihoods.copy()
            lh_test = le_test.predict_proba(x_test[:, i])

            if nclass <= 2:
                lh_train = lh_train.T[1].reshape(-1, 1)
                lh_test = lh_test.T[1].reshape(-1, 1)

            xx_train = np.hstack((lh_train,)) if xx_train is None else np.hstack((xx_train, lh_train))
            xx_test = np.hstack((lh_test,)) if xx_test is None else np.hstack((xx_test, lh_test))

        return xx_train, xx_test

    def fit_transform(self, x, y):
        self.fit(x, y)
        return self.transform(x)

    def transform(self, x):
        if len(x.shape) == 1:
            x = x.reshape(-1, 1)
        ncols = x.shape[1]
        if not is_numpy(x):
            x = np.array(x)

        likelihoods = None

        for i in range(ncols):
            lh = self.estimators[i].predict(x[:, i], noise=True).reshape(-1, 1)
            # lh = self.estimators[i].predict_proba(x[:, i])
            # if self.nclass <= 2:
            #     lh = lh.T[1].reshape(-1, 1)
            likelihoods = np.hstack((lh,)) if likelihoods is None else np.hstack((likelihoods, lh))
        return likelihoods


class PercentileEncoder(BaseEstimator, TransformerMixin):
    def __init__(self, apply_ppf=False, copy=True):
        self.ppf = lambda x: norm.ppf(x * .998 + .001) if apply_ppf else x
        self.copy = copy
        self.ecdfs = {}

    def fit(self, x):
        self.ecdfs = {}
        if len(x.shape) == 1:
            x = x.reshape(-1, 1)
        ncols = x.shape[1]
        is_np = is_numpy(x)

        for i in range(ncols):
            self.ecdfs.update({i: ECDF(x[:, i] if is_np else x.iloc[:, i].values)})
        return self

    def fit_transform(self, x):
        self.fit(x)
        return self.transform(x)

    def transform(self, x):
        if self.copy:
            x = x.copy()
        if len(x.shape) == 1:
            x = x.reshape(-1, 1)
        ncols = x.shape[1]
        is_np = is_numpy(x)

        for i in range(ncols):
            ecdf = self.ecdfs[i]
            if is_np:
                x[:, i] = self.ppf(ecdf(x[:, i]))
            else:
                x.iloc[:, i] = self.ppf(ecdf(x.iloc[:, i]))
        return x


class InfrequentValueEncoder(BaseEstimator, TransformerMixin):
    def __init__(self, threshold=10, value=-1, copy=True):
        self.threshold = threshold
        self.value = value
        self.copy = copy
        self.new_values = {}

    def fit(self, x):
        self.new_values = {}
        if len(x.shape) == 1:
            x = x.reshape(-1, 1)
        ncols = x.shape[1]
        is_np = is_numpy(x)

        for i in range(ncols):
            if is_np:
                val = dict(Counter(x[:, i]))
            else:
                val = x.iloc[:, i].value_counts().to_dict()
            val = dict((k, self.value if v < self.threshold else k) for k, v in val.items())
            self.new_values.update({i: val})
        return self

    def fit_transform(self, x):
        self.fit(x)
        return self.transform(x)

    def transform(self, x):
        if self.copy:
            x = x.copy()
        if len(x.shape) == 1:
            x = x.reshape(-1, 1)
        ncols = x.shape[1]
        is_np = is_numpy(x)

        for i in range(ncols):
            val = self.new_values[i]
            if is_np:
                k, v = np.array(list(zip(*sorted(val.items()))))
                ix = np.digitize(x[:, i], k, right=True)
                x[:, i] = v[ix]
            else:
                x.iloc[:, i].replace(val, inplace=True)
        return x


class CategoryEncoder(BaseEstimator, TransformerMixin):
    def __init__(self, min_count=0, first_category=1, copy=True):
        self.min_count = min_count
        self.first_category = first_category
        self.copy = copy
        self.encoders = {}
        self.ive = None

    def fit(self, x):
        self.encoders = {}
        if len(x.shape) == 1:
            x = x.reshape(-1, 1)
        ncols = x.shape[1]
        is_np = is_numpy(x)

        if self.min_count > 0:
            self.ive = InfrequentValueEncoder(threshold=self.min_count, value=np.finfo(float).min)
            x = self.ive.fit_transform(x)

        for i in range(ncols):
            if is_np:
                enc = LabelEncoder().fit(x[:, i])
            else:
                enc = LabelEncoder().fit(x.iloc[:, i])
            self.encoders.update({i: enc})
        return self

    def fit_transform(self, x):
        self.fit(x)
        return self.transform(x)

    def transform(self, x):
        if self.copy:
            x = x.copy()
        if len(x.shape) == 1:
            x = x.reshape(-1, 1)
        ncols = x.shape[1]
        is_np = is_numpy(x)

        if self.ive is not None:
            x = self.ive.transform(x)

        for i in range(ncols):
            enc = self.encoders[i]
            if is_np:
                x[:, i] = enc.transform(x[:, i]) + self.first_category
            else:
                x.iloc[:, i] = enc.transform(x.iloc[:, i]) + self.first_category
        return x


class DummyEncoder(BaseEstimator, TransformerMixin):
    pass
```

这个repo的其他内容也值得一看



ZCA whitening

一般在处理图像的时候用。参考[白化][1]

```python
# https://gist.github.com/erogol/9618975

import numpy as np
from scipy import linalg
from sklearn.utils import array2d, as_float_array
from sklearn.base import TransformerMixin, BaseEstimator


class ZCA(BaseEstimator, TransformerMixin):

    def __init__(self, regularization=10**-5, copy=False):
        self.regularization = regularization
        self.copy = copy

    def fit(self, X, y=None):
        X = array2d(X)
        X = as_float_array(X, copy = self.copy)
        self.mean_ = np.mean(X, axis=0)
        X -= self.mean_
        sigma = np.dot(X.T,X) / X.shape[1]
        U, S, V = linalg.svd(sigma)
        tmp = np.dot(U, np.diag(1/np.sqrt(S+self.regularization)))
        self.components_ = np.dot(tmp, U.T)
        return self

    def transform(self, X):
        X = array2d(X)
        X_transformed = X - self.mean_
        X_transformed = np.dot(X_transformed, self.components_.T)
        return X_transformed
```



pangram 项目的Transformer,  主要是主题模型的(?)

```python
# https://github.com/pan-webis-de/pangram/blob/master/pan/features.py
""" Module containing feature generators used for learning.
    I think I reinvented sklearn pipelines - too late now!
    A dictionary of functions is used for feature generation.
    If a function has only one argument feature generation is
    independent of training or test case.
    If it takes two arguments, feature generation depends
    on case - for example: bag_of_words
    This is supposed to be extensible as you can add or remove
    any functions you like from the dictionary
"""
import regex as re
import nltk
import numpy
from textblob.tokenizers import WordTokenizer
from sklearn.base import BaseEstimator, TransformerMixin

# ------------------------ feature generators --------------------------------#


class TopicTopWords(BaseEstimator, TransformerMixin):

    """ Suppose texts can be split into n topics. Represent each text
        as a percentage for each topic."""

    def __init__(self, n_topics, k_top):
        import lda
        from sklearn.feature_extraction.text import CountVectorizer
        self.n_topics = n_topics
        self.k_top = k_top
        self.model = lda.LDA(n_topics=self.n_topics,
                             n_iter=10,
                             random_state=1)
        self.counter = CountVectorizer()

    def fit(self, X, y=None):
        X = self.counter.fit_transform(X)
        self.model.fit(X)
        return self

    def transform(self, texts):
        """ transform data

        :texts: The texts to count hashes in
        :returns: list of counts for each text

        """
        X = self.counter.transform(texts).toarray()  # get counts for each word
        topic_words = self.model.topic_word_  # model.components_ also works
        topics = numpy.hstack([X[:, numpy.argsort(topic_dist)]
                                [:, :-(self.k_top+1):-1]
                              for topic_dist in topic_words])
        return topics


class CountHash(BaseEstimator, TransformerMixin):

    """ Model that extracts a counter of twitter-style hashes. """

    pat = re.compile(r'(?<=\s+|^)#\w+', re.UNICODE)

    def fit(self, X, y=None):
        return self

    def transform(self, texts):
        """ transform data

        :texts: The texts to count hashes in
        :returns: list of counts for each text

        """
        return [[len(CountHash.pat.findall(text))] for text in texts]


class CountReplies(BaseEstimator, TransformerMixin):

    """ Model that extracts a counter of twitter-style @replies. """

    pat = re.compile(r'(?<=\s+|^)@\w+', re.UNICODE)

    def fit(self, X, y=None):
        return self

    def transform(self, texts):
        """ transform data

        :texts: The texts to count replies in
        :returns: list of counts for each text

        """
        return [[len(CountReplies.pat.findall(text))] for text in texts]


class CountURLs(BaseEstimator, TransformerMixin):

    """ Model that extracts a counter of URL links from text. """

    pat = re.compile(r'((https?|ftp)://[^\s/$.?#].[^\s]*)')

    def fit(self, X, y=None):
        return self

    def transform(self, texts):
        """ transform data

        :texts: The texts to count URLs in
        :returns: list of counts for each text

        """
        return [[len(CountURLs.pat.findall(text))] for text in texts]


class CountCaps(BaseEstimator, TransformerMixin):

    """ Model that extracts a counter of capital letters from text. """

    def fit(self, X, y=None):
        return self

    def transform(self, texts):
        """ transform data

        :texts: The texts to count capital letters in
        :returns: list of counts for each text

        """
        return [[sum(c.isupper() for c in text)] for text in texts]


class CountWordCaps(BaseEstimator, TransformerMixin):

    """ Model that extracts a counter of capital words from text. """

    def fit(self, X, y=None):
        return self

    def transform(self, texts):
        """ transform data

        :texts: The texts to count capital words in
        :returns: list of counts for each text

        """
        return [[sum(w.isupper() for w in nltk.word_tokenize(text))]
                for text in texts]


class CountWordLength(BaseEstimator, TransformerMixin):

    """ Model that extracts a counter of word length from text. """

    def __init__(self, span):
        """ Initialize this feature extractor
        :span: tuple - range of lengths to count

        """
        self.span = span

    def fit(self, X, y=None):
        return self

    def transform(self, texts):
        """ transform data

        :texts: The texts to count word lengths in
        :returns: list of counts for each text

        """
        mini, maxi = self.span
        num_counts = maxi - mini
        wt = WordTokenizer()
        tokens = [wt.tokenize(text) for text in texts]
        text_len_dist = []
        for line_tokens in tokens:
            counter = [0]*num_counts
            for word in line_tokens:
                word_len = len(word)
                if mini <= word_len <= maxi:
                    counter[word_len - 1] += 1
            text_len_dist.append([each for each in counter])
        return text_len_dist
```



这里有另一个：

```python
# https://gist.github.com/kudkudak/b7f8e91b7e1cf8938af83b6f43e9d100
import sys
import numpy as np
from web import embedding
import web

from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.decomposition import RandomizedPCA

import logging

logger = logging.getLogger(__name__)

class SentenceEmbedder(BaseEstimator, TransformerMixin):
    """
        Transforms sentences into vector representation using word embeddings

        Notes
        -----
            remember about cleaning and lowering words to match embedding vocabulary!

            it is best if you don't change on_missing because it is a good test
            that your intentions are met (i.e. no words are skipped because you forgot
            to lower either Embedding or text dataset)

            web.embedding.Embedding is assumed because pandas .loc/iloc indexing can
            be very slow

            to be fair it is worth restrcing all embeddings to common vocabulary
    """

    def __init__(self, embedding, method="avg", on_missing="raise"):
        self.on_missing = on_missing

        if self.on_missing not in ['raise', 'skip']:
            raise NotImplementedError("Not implemented {} on_missing arg".format(self.on_missing))

        self.embedding = embedding
        if not isinstance(self.embedding, web.embedding.Embedding):
            raise NotImplementedError()

        self.method = method

        if self.method not in ['avg', 'concat']:
            raise NotImplementedError()

    def fit(self, X, y=None):
        return self

    def transform(self, X, y=None):
        # words for 'raise' and 'avg' parameters
        if self.method == 'avg':
            X_tr = np.zeros(shape=(X.shape[0], len(self.embedding.vectors[0])), dtype=self.embedding.vectors.dtype)
        elif self.method == "concat":
            X_tr = np.zeros(shape=(X.shape[0], len(X[0]) * len(self.embedding.vectors[0])), dtype=self.embedding.vectors.dtype)
            logger.info("Creating embedding via {} of shape {}".format(self.method, X_tr.shape))
            if not all(len(query) == len(X[0]) for query in X):
                raise RuntimeError("Variable sized sentences don't work with 'concat")
        else:
            raise NotImplementedError()

        skipped = 0
        for id, query in enumerate(X):
            query_tr = []
            for word in query:
                if word in self.embedding:
                    query_tr.append(self.embedding[word])
                else:
                    if self.on_missing == "raise":
                        raise RuntimeError("Not found word")
                    elif self.on_missing == "skip":
                        skipped += 1
                    else:
                        raise NotImplementedError()

            if self.method == "avg":
                X_tr[id] = np.mean(np.array(query_tr).T, axis=1).reshape(-1,)
            elif self.method == "concat":
                A = np.hstack([q.reshape(-1, ) for q in query_tr])
                assert len(A) == X_tr.shape[1], "Constructed correctly sized vector"
                X_tr[id] = A
            else:
                raise NotImplementedError()

        if skipped > 0:
            logger.warning("Skipped {} out of vocabulary words".format(skipped))

        return X_tr

class DotProductEmbedder(BaseEstimator, TransformerMixin):
    """
        Transforms sentence into dot products of required pairs

        pairs = [(0,1)] means join 0 and 1 vector via method
    """

    def __init__(self, embedding, method="diag", on_missing="raise", pairs=[[0,1]]):
        self.on_missing = on_missing
        self.pairs = pairs

        if self.on_missing not in ['raise', 'skip']:
            raise NotImplementedError("Not implemented {} on_missing arg".format(self.on_missing))

        self.embedding = embedding
        if not isinstance(self.embedding, web.embedding.Embedding):
            raise NotImplementedError()

        self.method = method

        if self.method not in ['diag']:
            raise NotImplementedError()

    def fit(self, X, y=None):
        return self

    def transform(self, X, y=None):
        # words for 'raise' and 'avg' parameters
        L = len(self.embedding.vectors[0])
        if self.method == 'diag':
            X_tr = np.zeros(shape=(X.shape[0], L * len(self.pairs)), dtype=self.embedding.vectors.dtype)
            if not all(len(query) == len(X[0]) for query in X):
                raise RuntimeError("Variable sized sentences don't work with 'diag")
        else:
            raise NotImplementedError()

        skipped = 0
        for id, query in enumerate(X):
            query_tr = []
            for word in query:
                if word in self.embedding:
                    query_tr.append(self.embedding[word])
                else:
                    if self.on_missing == "raise":
                        raise RuntimeError("Not found word")
                    elif self.on_missing == "skip":
                        skipped += 1
                    else:
                        raise NotImplementedError()

            if self.method == "diag":
                start_id = 0
                for p in self.pairs:
                    X_tr[id, start_id: (start_id + L)] = np.multiply(query_tr[p[0]], query_tr[p[1]])
                    start_id += L
            else:
                raise NotImplementedError()

        if skipped > 0:
            logger.warning("Skipped {} out of vocabulary words".format(skipped))

        return X_tr

class DoubleListEmbedder(BaseEstimator, TransformerMixin):
    """
        Transforms list of lists of words into list of lists of vectors
    """

    def __init__(self, embedding, on_missing="raise"):

        self.on_missing = on_missing
        if self.on_missing not in ['raise', 'skip']:
            raise NotImplementedError("Not implemented {} on_missing arg".format(self.on_missing))

        self.embedding = embedding
        if not isinstance(self.embedding, web.embedding.Embedding):
            raise NotImplementedError()

    def fit(self, X, y=None):
        return self

    def transform(self, X, y=None):

        result = []
        skipped = 0

        for list_of_words in X:

            list_of_vectors = []

            for word in list_of_words:
                if word in self.embedding:
                    list_of_vectors.append(self.embedding[word])
                else:
                    if self.on_missing == "raise":
                        raise RuntimeError("Not found word")
                    elif self.on_missing == "skip":
                        skipped += 1
                    else:
                        raise NotImplementedError()

            result.append(list_of_vectors)

        if skipped > 0:
            logger.warning("Skipped {} out of vocabulary words".format(skipped))

        return result


class DoubleListDotProduct(BaseEstimator, TransformerMixin):
    """
        Transforms list of lists of vectors into numpy array
        Currently only n x 2 input supported
    """

    def __init__(self, method="single", pairs=[[0,1]]):

        assert(method in ("single", "diagonal", "quadruple_diagonal", "double_diagonal", "all", "all_sym"))
        assert(pairs == [[0,1]])

        self.method = method
        self.pairs = pairs

    def fit(self, X, y=None):
        return self

    def transform(self, X, y=None):

        V = X[0][0]

        if self.method == "single":
            n_cols = 1
            joiner = lambda v, w: np.dot(v.reshape((1,-1)),w.reshape((-1,1))).ravel()
        elif self.method == "diagonal":
            n_cols = V.shape[0]
            joiner = lambda v, w: np.multiply(v,w).ravel()
        elif self.method == "double_diagonal":
            n_cols = V.shape[0] * 2
            joiner = lambda v, w: np.hstack([np.multiply(v,w).ravel(), np.log((1 + np.multiply(v,w).ravel())/2.)])
        elif self.method == "triple_diagonal":
            n_cols = V.shape[0] * 3
            joiner = lambda v, w: np.hstack([v, w, np.multiply(v,w).ravel()])
        elif self.method == "quadruple_diagonal":
            n_cols = V.shape[0] * 4
            joiner = lambda v, w: np.hstack([v, w, np.multiply(v,w).ravel(), np.log((1 + np.multiply(v,w).ravel())/2.)])
        elif self.method == "concat":
            n_cols = V.shape[0] * 2
            joiner = lambda v, w: np.hstack([v, w])
        elif self.method == "all":
            n_cols = V.shape[0] ** 2
            joiner = lambda v, w: np.dot(v.reshape((-1,1)),w.reshape((1,-1))).ravel()
        elif self.method == "all_sym":
            n_cols = V.shape[0] ** 2
            joiner = lambda v, w: ( ( np.dot(v.reshape((-1,1)),w.reshape((1,-1))) + np.dot(w.reshape((-1,1)),v.reshape((1,-1))) )/2 ).ravel()
        else:
            raise NotImplementedError()

        X_tr = np.zeros(shape=(len(X), n_cols), dtype=V.dtype)

        for i, pair in enumerate(X):
            X_tr[i,:] = joiner(pair[0], pair[1])

        return X_tr

class DoubleListConcat(BaseEstimator, TransformerMixin):
    """
        Transforms list of lists of vectors into numpy array
        Currently only n x k input supported
    """

    def __init__(self):
        pass

    def fit(self, X, y=None):
        return self

    def transform(self, X, y=None):

        how_many = len(X[0])
        V = X[0][0]
        X_tr = np.zeros((len(X), how_many*V.shape[0]), dtype=V.dtype)
        for i, vectors in enumerate(X):
            X_tr[i,:] = np.hstack(vectors)

        return X_tr
```



[1]: http://ufldl.stanford.edu/wiki/index.php/%E7%99%BD%E5%8C%96	"白化"

